{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from DPHuBERT.wav2vec2.model import wav2vec2_model\n",
    "import torch.nn as nn\n",
    "import fairseq\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"./DPHuBERT/checkpoints/DPHuBERT-sp0.75.pth\"\n",
    "ckpt = torch.load(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_hubert_model = wav2vec2_model(**ckpt[\"config\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSLModel(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(SSLModel, self).__init__()\n",
    "\n",
    "        cp_path = \"./checkpoints_xlsr/xlsr2_300m.pt\"  # Change the pre-trained XLSR model path.\n",
    "        model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task(\n",
    "            [cp_path]\n",
    "        )\n",
    "        self.model = model[0]\n",
    "        self.device = device\n",
    "        self.out_dim = 1024\n",
    "        return\n",
    "\n",
    "    def extract_feat(self, input_data):\n",
    "\n",
    "        # put the model to GPU if it not there\n",
    "        if (\n",
    "            next(self.model.parameters()).device != input_data.device\n",
    "            or next(self.model.parameters()).dtype != input_data.dtype\n",
    "        ):\n",
    "            self.model.to(input_data.device, dtype=input_data.dtype)\n",
    "            self.model.train()\n",
    "\n",
    "        if True:\n",
    "            # input should be in shape (batch, length)\n",
    "            if input_data.ndim == 3:\n",
    "                input_tmp = input_data[:, :, 0]\n",
    "            else:\n",
    "                input_tmp = input_data\n",
    "\n",
    "            # [batch, length, dim]\n",
    "            emb = self.model(input_tmp, mask=False, features_only=True)[\"x\"]\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data/a.varlamov/LJSpeech-1.1/wavs/LJ001-0008.wav\"\n",
    "audio, sr = torchaudio.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.cat([audio] * 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 39325])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsr_model = SSLModel(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xlsr_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32334/3872162863.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxlsr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_feat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'xlsr_model' is not defined"
     ]
    }
   ],
   "source": [
    "xlsr_model.extract_feat(batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 122, 768])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_hubert_model.extract_features(batch)[0][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPHubertModel(nn.Module):\n",
    "    def __init__(self, device, behaviour=\"last-layer\", freeze=False):\n",
    "        '''\n",
    "        Args:\n",
    "            device: obvious...\n",
    "            behaviour: last-layer / weighted-sum\n",
    "            freeze: to freeze weights of the pre-train or not\n",
    "                for weighted-sum freezing will not let weights of sum train\n",
    "        '''\n",
    "        super(DPHubertModel, self).__init__()\n",
    "\n",
    "        ckpt_path = \"./DPHuBERT/checkpoints/DPHuBERT-sp0.75.pth\"\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "        self.model = wav2vec2_model(**ckpt[\"config\"]).to(device)\n",
    "        self.device = device\n",
    "        self.out_dim = 768\n",
    "        self.n_layers = 12\n",
    "        self.behaviour = behaviour\n",
    "        \n",
    "        if behaviour == \"weighted-sum\":\n",
    "            self.sum_weights = nn.parameter.Parameter(torch.tensor([0.] * 9 + [0.5, 0.5, 0.5])).reshape(self.n_layers, 1, 1, 1)\n",
    "        \n",
    "        if freeze:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def extract_feat(self, input_data):\n",
    "\n",
    "        # put the model to GPU if it not there\n",
    "        if (\n",
    "            next(self.model.parameters()).device != input_data.device\n",
    "            or next(self.model.parameters()).dtype != input_data.dtype\n",
    "        ):\n",
    "            self.model.to(input_data.device, dtype=input_data.dtype)\n",
    "            self.model.train()\n",
    "\n",
    "        if True:\n",
    "            # input should be in shape (batch, length)\n",
    "            if input_data.ndim == 3:\n",
    "                input_tmp = input_data[:, :, 0]\n",
    "            else:\n",
    "                input_tmp = input_data\n",
    "\n",
    "            # [batch, length, dim]\n",
    "            if self.behaviour == \"last-layer\":\n",
    "                emb = self.model.extract_features(input_tmp)[0][-1]  # getting features from the last layer of transformer\n",
    "            elif self.behaviour == \"weighted-sum\":\n",
    "                all_layers_out = self.model.extract_features(input_tmp)[0][1:]\n",
    "                all_layers_out = torch.stack(all_layers_out)\n",
    "                emb = (all_layers_out * self.sum_weights).sum(dim=0)\n",
    "                return emb\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_hubert_model = DPHubertModel(\"cpu\", freeze=False, behaviour=\"weighted-sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = dp_hubert_model.extract_feat(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 2, 122, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(all_out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list = [torch.randn(2, 122, 768) for _ in range(12)]  # list of 12 tensors\n",
    "weights_tensor = torch.randn(12)  # 1D tensor with 12 elements\n",
    "\n",
    "# Convert list of tensors into a single tensor of shape [12, 2, 122, 768]\n",
    "tensor_stack = torch.stack(tensor_list)  # Shape: [12, 2, 122, 768]\n",
    "\n",
    "# Reshape weights_tensor to [12, 1, 1, 1] for broadcasting\n",
    "weights_tensor_reshaped = weights_tensor.view(12, 1, 1, 1)\n",
    "\n",
    "# Perform element-wise multiplication\n",
    "result = (tensor_stack * weights_tensor_reshaped).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 122, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# wav2vec XLSR works faster then DPHubert, which is more than 10 times smaller!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 58s, sys: 8min 15s, total: 16min 14s\n",
      "Wall time: 7.98 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.1684e-01, -1.3144e-01, -9.9839e-02,  ..., -2.2259e-02,\n",
       "           2.6934e-02,  3.4119e-01],\n",
       "         [-3.1124e-02, -7.2893e-02,  4.6221e-02,  ..., -9.5193e-02,\n",
       "           8.7276e-02,  5.3428e-01],\n",
       "         [ 6.8431e-02, -4.2156e-02,  7.4829e-02,  ..., -1.1923e-01,\n",
       "           7.5676e-02,  6.3278e-01],\n",
       "         ...,\n",
       "         [ 1.0732e-01,  4.2675e-02,  1.1723e-01,  ..., -1.0207e-01,\n",
       "           1.0415e-01,  6.0296e-01],\n",
       "         [-3.0037e-02,  5.5227e-04,  7.0944e-02,  ..., -9.7161e-02,\n",
       "           1.1185e-01,  4.5757e-01],\n",
       "         [-2.8352e-01, -6.8307e-02, -6.7748e-02,  ..., -3.2461e-02,\n",
       "           3.2547e-02,  2.2996e-01]],\n",
       "\n",
       "        [[-3.1684e-01, -1.3144e-01, -9.9839e-02,  ..., -2.2259e-02,\n",
       "           2.6934e-02,  3.4119e-01],\n",
       "         [-3.1124e-02, -7.2892e-02,  4.6222e-02,  ..., -9.5193e-02,\n",
       "           8.7275e-02,  5.3428e-01],\n",
       "         [ 6.8431e-02, -4.2156e-02,  7.4829e-02,  ..., -1.1923e-01,\n",
       "           7.5676e-02,  6.3278e-01],\n",
       "         ...,\n",
       "         [ 1.0732e-01,  4.2675e-02,  1.1723e-01,  ..., -1.0207e-01,\n",
       "           1.0415e-01,  6.0296e-01],\n",
       "         [-3.0036e-02,  5.5283e-04,  7.0943e-02,  ..., -9.7160e-02,\n",
       "           1.1185e-01,  4.5757e-01],\n",
       "         [-2.8352e-01, -6.8307e-02, -6.7748e-02,  ..., -3.2461e-02,\n",
       "           3.2547e-02,  2.2996e-01]],\n",
       "\n",
       "        [[-3.1684e-01, -1.3144e-01, -9.9839e-02,  ..., -2.2260e-02,\n",
       "           2.6934e-02,  3.4119e-01],\n",
       "         [-3.1124e-02, -7.2892e-02,  4.6222e-02,  ..., -9.5194e-02,\n",
       "           8.7274e-02,  5.3428e-01],\n",
       "         [ 6.8431e-02, -4.2156e-02,  7.4830e-02,  ..., -1.1924e-01,\n",
       "           7.5675e-02,  6.3278e-01],\n",
       "         ...,\n",
       "         [ 1.0732e-01,  4.2676e-02,  1.1723e-01,  ..., -1.0207e-01,\n",
       "           1.0415e-01,  6.0296e-01],\n",
       "         [-3.0037e-02,  5.5289e-04,  7.0944e-02,  ..., -9.7161e-02,\n",
       "           1.1185e-01,  4.5757e-01],\n",
       "         [-2.8352e-01, -6.8307e-02, -6.7747e-02,  ..., -3.2461e-02,\n",
       "           3.2547e-02,  2.2996e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-3.1684e-01, -1.3144e-01, -9.9839e-02,  ..., -2.2259e-02,\n",
       "           2.6934e-02,  3.4119e-01],\n",
       "         [-3.1124e-02, -7.2893e-02,  4.6221e-02,  ..., -9.5193e-02,\n",
       "           8.7276e-02,  5.3428e-01],\n",
       "         [ 6.8431e-02, -4.2157e-02,  7.4828e-02,  ..., -1.1923e-01,\n",
       "           7.5677e-02,  6.3278e-01],\n",
       "         ...,\n",
       "         [ 1.0732e-01,  4.2675e-02,  1.1723e-01,  ..., -1.0207e-01,\n",
       "           1.0415e-01,  6.0296e-01],\n",
       "         [-3.0037e-02,  5.5188e-04,  7.0943e-02,  ..., -9.7160e-02,\n",
       "           1.1185e-01,  4.5757e-01],\n",
       "         [-2.8352e-01, -6.8308e-02, -6.7748e-02,  ..., -3.2461e-02,\n",
       "           3.2548e-02,  2.2996e-01]],\n",
       "\n",
       "        [[-3.1684e-01, -1.3144e-01, -9.9839e-02,  ..., -2.2259e-02,\n",
       "           2.6934e-02,  3.4119e-01],\n",
       "         [-3.1125e-02, -7.2892e-02,  4.6222e-02,  ..., -9.5193e-02,\n",
       "           8.7275e-02,  5.3428e-01],\n",
       "         [ 6.8431e-02, -4.2156e-02,  7.4829e-02,  ..., -1.1923e-01,\n",
       "           7.5676e-02,  6.3278e-01],\n",
       "         ...,\n",
       "         [ 1.0732e-01,  4.2675e-02,  1.1723e-01,  ..., -1.0207e-01,\n",
       "           1.0415e-01,  6.0296e-01],\n",
       "         [-3.0037e-02,  5.5204e-04,  7.0943e-02,  ..., -9.7160e-02,\n",
       "           1.1185e-01,  4.5757e-01],\n",
       "         [-2.8352e-01, -6.8307e-02, -6.7748e-02,  ..., -3.2462e-02,\n",
       "           3.2547e-02,  2.2996e-01]],\n",
       "\n",
       "        [[-3.1684e-01, -1.3144e-01, -9.9839e-02,  ..., -2.2259e-02,\n",
       "           2.6934e-02,  3.4119e-01],\n",
       "         [-3.1124e-02, -7.2893e-02,  4.6221e-02,  ..., -9.5193e-02,\n",
       "           8.7276e-02,  5.3428e-01],\n",
       "         [ 6.8431e-02, -4.2156e-02,  7.4829e-02,  ..., -1.1923e-01,\n",
       "           7.5676e-02,  6.3278e-01],\n",
       "         ...,\n",
       "         [ 1.0732e-01,  4.2675e-02,  1.1723e-01,  ..., -1.0207e-01,\n",
       "           1.0415e-01,  6.0296e-01],\n",
       "         [-3.0037e-02,  5.5227e-04,  7.0944e-02,  ..., -9.7161e-02,\n",
       "           1.1185e-01,  4.5757e-01],\n",
       "         [-2.8352e-01, -6.8307e-02, -6.7748e-02,  ..., -3.2461e-02,\n",
       "           3.2547e-02,  2.2996e-01]]], grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "xlsr_model.extract_feat(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 2s, sys: 46.8 s, total: 2min 49s\n",
      "Wall time: 2.25 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3416,  1.8239,  2.2060,  ...,  0.6883,  1.1687,  0.0100],\n",
       "         [-0.5105,  1.5057,  0.5117,  ..., -0.2433, -0.6276, -0.4823],\n",
       "         [ 0.8634, -0.2174,  0.8490,  ..., -0.0038, -0.2467, -1.8248],\n",
       "         ...,\n",
       "         [-0.8037,  3.2378,  0.4703,  ...,  0.1115, -1.5998,  0.4930],\n",
       "         [-1.2858,  1.7123,  0.8499,  ..., -0.8332, -0.9068,  0.0979],\n",
       "         [-1.0495,  2.3076,  0.5729,  ..., -1.8239, -0.9515,  1.3305]],\n",
       "\n",
       "        [[-1.1864,  0.9392,  2.9589,  ...,  0.6217, -0.3574,  1.2964],\n",
       "         [-1.1078,  0.6414,  2.1782,  ..., -0.3556,  0.0243, -1.0702],\n",
       "         [ 0.8445,  1.0507, -0.0178,  ...,  0.2536, -0.3710, -1.5432],\n",
       "         ...,\n",
       "         [-0.3000,  1.9700, -0.2216,  ..., -0.6348, -1.2263, -0.3300],\n",
       "         [ 0.2855,  1.4401,  0.8531,  ..., -1.4564, -1.0461,  0.6909],\n",
       "         [-0.3108,  2.3947,  0.1691,  ..., -0.1243, -1.5212, -0.4023]],\n",
       "\n",
       "        [[-0.8744,  0.4075,  2.9414,  ...,  0.9608, -0.6673, -0.1667],\n",
       "         [-1.0468,  0.6904,  1.7022,  ..., -0.6009, -0.0390, -0.4691],\n",
       "         [ 1.0756,  0.4251,  0.3717,  ..., -0.1843, -0.6644, -3.0542],\n",
       "         ...,\n",
       "         [-0.3376,  1.1697,  0.4355,  ..., -0.4856, -1.8558,  0.8131],\n",
       "         [-0.8332,  1.6652,  1.1448,  ...,  0.2603,  0.2310,  0.3976],\n",
       "         [-2.1152,  2.1189,  0.7543,  ..., -0.0652, -1.0574,  0.0545]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.2449,  0.9760,  2.5140,  ...,  0.5442, -0.1145,  0.7444],\n",
       "         [-1.0243,  0.8526,  1.5003,  ...,  0.1957, -0.2313, -0.9008],\n",
       "         [ 0.3111,  0.8525,  1.0360,  ..., -0.5690, -0.8190, -1.5428],\n",
       "         ...,\n",
       "         [-1.0118,  1.1074,  0.3966,  ..., -0.4857, -1.6114,  0.5836],\n",
       "         [ 0.3858,  1.7938,  0.3572,  ...,  0.7228, -1.7129,  0.9694],\n",
       "         [-1.0469,  1.1291,  2.2247,  ...,  0.9695, -1.0779, -0.2283]],\n",
       "\n",
       "        [[-0.9553,  1.2343,  1.1185,  ..., -0.4008, -0.0986,  0.1158],\n",
       "         [-1.5376,  1.0136,  0.5868,  ..., -0.5209, -1.2809, -0.5901],\n",
       "         [-0.6773,  0.4005,  0.0733,  ...,  0.5314, -0.7967, -1.6455],\n",
       "         ...,\n",
       "         [-1.2346,  0.2009,  0.8817,  ...,  0.5056, -0.7220, -0.1771],\n",
       "         [-0.1205,  1.0884, -0.5115,  ...,  0.4148, -2.3824,  0.8014],\n",
       "         [-0.4845,  1.4627,  0.2804,  ...,  1.2730, -0.4891,  1.0824]],\n",
       "\n",
       "        [[-1.4543,  0.7457,  1.8437,  ...,  0.8609, -0.4469, -0.1524],\n",
       "         [-1.4996,  1.6041,  1.3126,  ...,  1.5829, -0.7713,  0.0881],\n",
       "         [ 0.2718, -0.0540,  0.4797,  ...,  0.9911, -0.1060, -0.4318],\n",
       "         ...,\n",
       "         [-1.8322,  1.1201,  1.5621,  ...,  1.5890, -1.5114,  0.5510],\n",
       "         [-1.4071,  1.0977,  0.9061,  ..., -0.3088, -1.1267,  0.9797],\n",
       "         [-1.1803,  1.4662,  1.3155,  ..., -0.5440, -0.3999, -0.3131]]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dp_hubert_model.extract_feat(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34min 43s, sys: 3min 31s, total: 38min 14s\n",
      "Wall time: 22.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(32):\n",
    "    xlsr_model.extract_feat(batch[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 38s, sys: 51.6 s, total: 18min 30s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(32):\n",
    "    dp_hubert_model.extract_feat(batch[0].unsqueeze(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssl_aasist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
